{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itIJtXj0goF0"
      },
      "source": [
        "# Module 7 - AI Agents for Data Engineering\n",
        "# Session 1: Building Your First AI Agent\n",
        "\n",
        "## Welcome!\n",
        "\n",
        "Today you're going to build AI agents from scratch using Google's Gemini API and Python.\n",
        "\n",
        "By the end of this session, you'll have:\n",
        "- ‚úì Connected to a Large Language Model (LLM)\n",
        "- ‚úì Made your first API call\n",
        "- ‚úì Built a conversational AI agent\n",
        "- ‚úì Customised it for data engineering tasks\n",
        "\n",
        "**Important:** Save this notebook to your Google Drive so you can continue experimenting later!\n",
        "\n",
        "---\n",
        "\n",
        "## Part 1: Setup Your API Key (Secure Method)\n",
        "\n",
        "First, we need to configure access to Google's Gemini API.\n",
        "\n",
        "**Steps:**\n",
        "1. Open a new browser tab: https://aistudio.google.com\n",
        "2. Sign in with your Google account\n",
        "3. Click \"Get API key\" ‚Üí \"Create API key in new project\"\n",
        "4. Copy the key (it starts with \"AIza...\")\n",
        "5. Come back to this notebook\n",
        "\n",
        "**Now we'll store it securely in Colab:**\n",
        "\n",
        "üëà Look at the left sidebar - click the **key icon** (üîë)\n",
        "\n",
        "Then:\n",
        "- Click \"Add new secret\"\n",
        "- Name: `GEMINI_API_KEY`\n",
        "- Value: Paste your API key\n",
        "- Toggle \"Notebook access\" ON\n",
        "\n",
        "**Check:** You should see a green checkmark next to \"Notebook access\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zqq5zuE4goF1",
        "outputId": "808b6e80-ea36-4425-d954-bb9dc678b968"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì API key found!\n",
            "‚úì Key starts with: AIzaSyABGV...\n"
          ]
        }
      ],
      "source": [
        "# Let's verify your API key is configured correctly\n",
        "from google.colab import userdata\n",
        "\n",
        "try:\n",
        "    api_key = userdata.get('GEMINI_API_KEY')\n",
        "    print(\"‚úì API key found!\")\n",
        "    print(f\"‚úì Key starts with: {api_key[:10]}...\")\n",
        "except Exception as e:\n",
        "    print(\"‚úó API key not found. Please add it using the key icon on the left.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRIVnSGPgoF2"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 2: Your First LLM Call\n",
        "\n",
        "Let's make the simplest possible call to an LLM and see what happens.\n",
        "\n",
        "**What we're doing:**\n",
        "- Installing the Gemini Python library\n",
        "- Creating a model connection\n",
        "- Sending a prompt\n",
        "- Getting a response\n",
        "\n",
        "It's that simple!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "5aHdv3dFgoF2"
      },
      "outputs": [],
      "source": [
        "# Install and import the Gemini library\n",
        "#!pip install -q google-generativeai\n",
        "\n",
        "import google.generativeai as genai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "gIRjET4DgoF2",
        "outputId": "2a587193-7c0a-4da4-d633-5d73b5f18fb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A data warehouse is a central repository that integrates and stores historical data from various operational sources, optimized for analytical reporting and business intelligence.\n"
          ]
        }
      ],
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "# Configure with your API key\n",
        "genai.configure(api_key=userdata.get(\"GEMINI_API_KEY\"))\n",
        "\n",
        "# Create model instance\n",
        "model = genai.GenerativeModel('gemini-2.5-flash')\n",
        "\n",
        "# Generate response\n",
        "response = model.generate_content(\"Explain what a data warehouse is in one sentence\")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gp4sIyObgoF3"
      },
      "source": [
        "### üéØ Your Turn: Experiment!\n",
        "\n",
        "**Task:** Modify the prompt above to ask a data engineering question relevant to YOUR work.\n",
        "\n",
        "Try asking about:\n",
        "- SQL concepts\n",
        "- ETL processes\n",
        "- Data quality\n",
        "- Database design\n",
        "- Python data processing\n",
        "\n",
        "Run the cell multiple times with different questions. See what you get!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6efiMO3xgoF3"
      },
      "outputs": [],
      "source": [
        "# Try your own questions here\n",
        "# Copy the code from above, change the prompt, and run it!\n",
        "\n",
        "model = genai.GenerativeModel('gemini-2.0-flash-exp')\n",
        "\n",
        "# TODO: Change this to your question\n",
        "response = model.generate_content(\"YOUR QUESTION HERE\")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNZ3JOkjgoF3"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 3: Build a Conversational Agent\n",
        "\n",
        "That single question-answer is nice, but what if we want the agent to **remember** what we talked about?\n",
        "\n",
        "That's called **conversation history** - and it's what makes an LLM feel like an agent.\n",
        "\n",
        "**How it works:**\n",
        "- We start a \"chat\" instead of single generation\n",
        "- Each message remembers previous messages\n",
        "- The LLM can reference earlier parts of the conversation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJgPZQ_LgoF3"
      },
      "outputs": [],
      "source": [
        "# Start a new chat with conversation history\n",
        "chat = model.start_chat(history=[])\n",
        "\n",
        "# Send first message\n",
        "response = chat.send_message(\"I'm learning about ETL processes\")\n",
        "print(\"Agent:\", response.text)\n",
        "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "# Send follow-up - notice it remembers we're talking about ETL\n",
        "response = chat.send_message(\"What's the difference between ETL and ELT?\")\n",
        "print(\"Agent:\", response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RolzgKEgoF3"
      },
      "source": [
        "### Notice what happened?\n",
        "\n",
        "The agent understood that \"the difference\" referred to ETL vs ELT **because it remembered the first message**.\n",
        "\n",
        "Without conversation history, it wouldn't have known what we were comparing!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2M_6KL2goF4"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 4: Interactive Conversational Agent\n",
        "\n",
        "Let's make this more useful - an agent you can actually chat with!\n",
        "\n",
        "**Run the cell below and have a conversation about data engineering topics.**\n",
        "\n",
        "Test if it:\n",
        "- Remembers what you said earlier\n",
        "- Provides helpful code examples\n",
        "- Answers follow-up questions correctly\n",
        "\n",
        "Type `quit` to exit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7bE7pQXHgoF4"
      },
      "outputs": [],
      "source": [
        "# Create a fresh chat for our interactive session\n",
        "chat = model.start_chat(history=[])\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"ü§ñ DATA ENGINEERING ASSISTANT\")\n",
        "print(\"=\"*80)\n",
        "print(\"Ask me anything about data engineering!\")\n",
        "print(\"Topics: SQL, ETL, Python, Data Quality, Pipelines, Warehousing...\")\n",
        "print(\"\\nType 'quit' to exit\")\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "while True:\n",
        "    # Get user input\n",
        "    user_input = input(\"You: \")\n",
        "\n",
        "    # Exit condition\n",
        "    if user_input.lower() in ['quit', 'exit', 'bye']:\n",
        "        print(\"\\nüëã Thanks for chatting! Your agent is saved in this notebook.\")\n",
        "        break\n",
        "\n",
        "    # Send message and get response\n",
        "    try:\n",
        "        response = chat.send_message(user_input)\n",
        "        print(f\"\\nü§ñ Agent: {response.text}\\n\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        print(\"Try rephrasing your question.\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZ4aAFjqgoF4"
      },
      "source": [
        "---\n",
        "\n",
        "## üéØ Challenge: Test Your Agent's Memory\n",
        "\n",
        "Try this conversation sequence to test if your agent remembers context:\n",
        "\n",
        "1. \"I have a table called customers with columns: id, name, email, created_at\"\n",
        "2. \"How do I find duplicate emails?\"\n",
        "3. \"What if I want to keep the oldest record?\"\n",
        "\n",
        "**Did the agent remember your table structure?**\n",
        "\n",
        "Try other multi-turn conversations relevant to your work!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqalWAjHgoF4"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 5: What We've Built\n",
        "\n",
        "In this session you've created:\n",
        "\n",
        "‚úì **LLM Connection** - You can call Gemini API  \n",
        "‚úì **Single-turn Generation** - Ask questions, get answers  \n",
        "‚úì **Conversational Agent** - Remembers conversation history  \n",
        "‚úì **Interactive Interface** - Chat with your agent  \n",
        "\n",
        "**This is the foundation for all AI agents!**\n",
        "\n",
        "Everything else builds on these basics:\n",
        "- System prompts (Session 2)\n",
        "- Tool use (Session 3)\n",
        "- Multi-agent systems (Session 4)\n",
        "\n",
        "---\n",
        "\n",
        "## üìù Key Concepts\n",
        "\n",
        "**LLM (Large Language Model):**\n",
        "- Pre-trained AI model that understands and generates text\n",
        "- Examples: Gemini, GPT, Claude\n",
        "\n",
        "**Prompt:**\n",
        "- The input you send to the LLM\n",
        "- Can be a question, instruction, or context\n",
        "\n",
        "**Conversation History:**\n",
        "- Storing previous messages in the chat\n",
        "- Allows the agent to reference earlier context\n",
        "\n",
        "**Agent:**\n",
        "- An LLM with specific behaviour/purpose\n",
        "- Can be specialised through system prompts and tools\n",
        "\n",
        "---\n",
        "\n",
        "## üöÄ Next Steps\n",
        "\n",
        "**Before Session 2:**\n",
        "- Think about what data engineering tasks you'd like to automate\n",
        "- What would be useful in your actual work?\n",
        "\n",
        "**In Session 2 we'll:**\n",
        "- Add system prompts to specialise your agent\n",
        "- Create domain-specific agents (SQL helper, data quality checker, etc.)\n",
        "- Make agents give better, more focused responses\n",
        "\n",
        "---\n",
        "\n",
        "## üíæ Save This Notebook!\n",
        "\n",
        "**File ‚Üí Save a copy in Drive**\n",
        "\n",
        "Your API key is saved securely, and you can continue experimenting anytime!\n",
        "\n",
        "---\n",
        "\n",
        "## ü§î Reflection Questions\n",
        "\n",
        "Before we finish, think about:\n",
        "\n",
        "1. **What surprised you** about how simple this is?\n",
        "2. **Where in your work** could a conversational agent help?\n",
        "3. **What questions** would you want to ask your ideal data engineering assistant?\n",
        "\n",
        "Discuss with a colleague or make notes for yourself.\n",
        "\n",
        "---\n",
        "\n",
        "**Great work! See you in Session 2 where we'll make these agents much more powerful.** üéâ"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}