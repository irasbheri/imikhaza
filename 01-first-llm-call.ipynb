{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itIJtXj0goF0"
      },
      "source": [
        "# Module 7 - AI Agents for Data Engineering\n",
        "# Session 1: Building Your First AI Agent\n",
        "\n",
        "## Welcome!\n",
        "\n",
        "Today you're going to build AI agents from scratch using Google's Gemini API and Python.\n",
        "\n",
        "By the end of this session, you'll have:\n",
        "- ‚úì Connected to a Large Language Model (LLM)\n",
        "- ‚úì Made your first API call\n",
        "- ‚úì Built a conversational AI agent\n",
        "- ‚úì Customised it for data engineering tasks\n",
        "\n",
        "**Important:** Save this notebook to your Google Drive so you can continue experimenting later!\n",
        "\n",
        "---\n",
        "\n",
        "## Part 1: Setup Your API Key (Secure Method)\n",
        "\n",
        "First, we need to configure access to Google's Gemini API.\n",
        "\n",
        "**Steps:**\n",
        "1. Open a new browser tab: https://aistudio.google.com\n",
        "2. Sign in with your Google account\n",
        "3. Click \"Get API key\" ‚Üí \"Create API key in new project\"\n",
        "4. Copy the key (it starts with \"AIza...\")\n",
        "5. Come back to this notebook\n",
        "\n",
        "**Now we'll store it securely in Colab:**\n",
        "\n",
        "üëà Look at the left sidebar - click the **key icon** (üîë)\n",
        "\n",
        "Then:\n",
        "- Click \"Add new secret\"\n",
        "- Name: `GEMINI_API_KEY`\n",
        "- Value: Paste your API key\n",
        "- Toggle \"Notebook access\" ON\n",
        "\n",
        "**Check:** You should see a green checkmark next to \"Notebook access\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zqq5zuE4goF1",
        "outputId": "808b6e80-ea36-4425-d954-bb9dc678b968",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì API key found!\n",
            "‚úì Key starts with: AIzaSyABGV...\n"
          ]
        }
      ],
      "source": [
        "# Let's verify your API key is configured correctly\n",
        "from google.colab import userdata\n",
        "\n",
        "try:\n",
        "    api_key = userdata.get('GEMINI_API_KEY')\n",
        "    print(\"‚úì API key found!\")\n",
        "    print(f\"‚úì Key starts with: {api_key[:10]}...\")\n",
        "except Exception as e:\n",
        "    print(\"‚úó API key not found. Please add it using the key icon on the left.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRIVnSGPgoF2"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 2: Your First LLM Call\n",
        "\n",
        "Let's make the simplest possible call to an LLM and see what happens.\n",
        "\n",
        "**What we're doing:**\n",
        "- Installing the Gemini Python library\n",
        "- Creating a model connection\n",
        "- Sending a prompt\n",
        "- Getting a response\n",
        "\n",
        "It's that simple!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "5aHdv3dFgoF2"
      },
      "outputs": [],
      "source": [
        "# Install and import the Gemini library\n",
        "!pip install -q google-generativeai\n",
        "\n",
        "import google as genai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "gIRjET4DgoF2",
        "outputId": "3ac8e0cc-376a-445f-b7cb-10bdaf6cc734",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:404 POST /v1beta/models/gemini-2.0-flash-exp:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 457.23ms\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NotFound",
          "evalue": "404 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?%24alt=json%3Benum-encoding%3Dint: models/gemini-2.0-flash-exp is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotFound\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3855585169.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Generate a response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Explain what a data warehouse is in one sentence\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Print the result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/generativeai/generative_models.py\u001b[0m in \u001b[0;36mgenerate_content\u001b[0;34m(self, contents, generation_config, safety_settings, stream, tools, tool_config, request_options)\u001b[0m\n\u001b[1;32m    329\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mgeneration_types\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGenerateContentResponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m                 response = self._client.generate_content(\n\u001b[0m\u001b[1;32m    332\u001b[0m                     \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m                     \u001b[0;34m**\u001b[0m\u001b[0mrequest_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py\u001b[0m in \u001b[0;36mgenerate_content\u001b[0;34m(self, request, model, contents, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m         \u001b[0;31m# Send the request.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 835\u001b[0;31m         response = rpc(\n\u001b[0m\u001b[1;32m    836\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m             \u001b[0mretry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/gapic_v1/method.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maximum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiplier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_multiplier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m             )\n\u001b[0;32m--> 294\u001b[0;31m             return retry_target(\n\u001b[0m\u001b[1;32m    295\u001b[0m                 \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predicate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;31m# defer to shared logic for handling errors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m             next_sleep = _retry_error_helper(\n\u001b[0m\u001b[1;32m    157\u001b[0m                 \u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m                 \u001b[0mdeadline\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/retry/retry_base.py\u001b[0m in \u001b[0;36m_retry_error_helper\u001b[0;34m(exc, deadline, sleep_iterator, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0moriginal_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         )\n\u001b[0;32m--> 214\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfinal_exc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msource_exc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mon_error_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0mon_error_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misawaitable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ASYNC_RETRY_WARNING\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/timeout.py\u001b[0m in \u001b[0;36mfunc_with_timeout\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"timeout\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremaining_timeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc_with_timeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/grpc_helpers.py\u001b[0m in \u001b[0;36merror_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0merror_remapped_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallable_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRpcError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_grpc_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/rest.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, request, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m   1159\u001b[0m             \u001b[0;31m# subclass.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m400\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1161\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mcore_exceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_http_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m             \u001b[0;31m# Return the response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotFound\u001b[0m: 404 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?%24alt=json%3Benum-encoding%3Dint: models/gemini-2.0-flash-exp is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods."
          ]
        }
      ],
      "source": [
        "# Configure the API with your key\n",
        "genai.configure(api_key=userdata.get('GEMINI_API_KEY'))\n",
        "\n",
        "# Create a model instance\n",
        "# We're using 'gemini-2.0-flash-exp' - it's fast and free!\n",
        "model = genai.GenerativeModel('gemini-2.0-flash-exp')\n",
        "\n",
        "# Generate a response\n",
        "response = model.generate_content(\"Explain what a data warehouse is in one sentence\")\n",
        "\n",
        "# Print the result\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gp4sIyObgoF3"
      },
      "source": [
        "### üéØ Your Turn: Experiment!\n",
        "\n",
        "**Task:** Modify the prompt above to ask a data engineering question relevant to YOUR work.\n",
        "\n",
        "Try asking about:\n",
        "- SQL concepts\n",
        "- ETL processes\n",
        "- Data quality\n",
        "- Database design\n",
        "- Python data processing\n",
        "\n",
        "Run the cell multiple times with different questions. See what you get!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6efiMO3xgoF3"
      },
      "outputs": [],
      "source": [
        "# Try your own questions here\n",
        "# Copy the code from above, change the prompt, and run it!\n",
        "\n",
        "model = genai.GenerativeModel('gemini-2.0-flash-exp')\n",
        "\n",
        "# TODO: Change this to your question\n",
        "response = model.generate_content(\"YOUR QUESTION HERE\")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNZ3JOkjgoF3"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 3: Build a Conversational Agent\n",
        "\n",
        "That single question-answer is nice, but what if we want the agent to **remember** what we talked about?\n",
        "\n",
        "That's called **conversation history** - and it's what makes an LLM feel like an agent.\n",
        "\n",
        "**How it works:**\n",
        "- We start a \"chat\" instead of single generation\n",
        "- Each message remembers previous messages\n",
        "- The LLM can reference earlier parts of the conversation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJgPZQ_LgoF3"
      },
      "outputs": [],
      "source": [
        "# Start a new chat with conversation history\n",
        "chat = model.start_chat(history=[])\n",
        "\n",
        "# Send first message\n",
        "response = chat.send_message(\"I'm learning about ETL processes\")\n",
        "print(\"Agent:\", response.text)\n",
        "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "# Send follow-up - notice it remembers we're talking about ETL\n",
        "response = chat.send_message(\"What's the difference between ETL and ELT?\")\n",
        "print(\"Agent:\", response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RolzgKEgoF3"
      },
      "source": [
        "### Notice what happened?\n",
        "\n",
        "The agent understood that \"the difference\" referred to ETL vs ELT **because it remembered the first message**.\n",
        "\n",
        "Without conversation history, it wouldn't have known what we were comparing!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2M_6KL2goF4"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 4: Interactive Conversational Agent\n",
        "\n",
        "Let's make this more useful - an agent you can actually chat with!\n",
        "\n",
        "**Run the cell below and have a conversation about data engineering topics.**\n",
        "\n",
        "Test if it:\n",
        "- Remembers what you said earlier\n",
        "- Provides helpful code examples\n",
        "- Answers follow-up questions correctly\n",
        "\n",
        "Type `quit` to exit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7bE7pQXHgoF4"
      },
      "outputs": [],
      "source": [
        "# Create a fresh chat for our interactive session\n",
        "chat = model.start_chat(history=[])\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"ü§ñ DATA ENGINEERING ASSISTANT\")\n",
        "print(\"=\"*80)\n",
        "print(\"Ask me anything about data engineering!\")\n",
        "print(\"Topics: SQL, ETL, Python, Data Quality, Pipelines, Warehousing...\")\n",
        "print(\"\\nType 'quit' to exit\")\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "while True:\n",
        "    # Get user input\n",
        "    user_input = input(\"You: \")\n",
        "\n",
        "    # Exit condition\n",
        "    if user_input.lower() in ['quit', 'exit', 'bye']:\n",
        "        print(\"\\nüëã Thanks for chatting! Your agent is saved in this notebook.\")\n",
        "        break\n",
        "\n",
        "    # Send message and get response\n",
        "    try:\n",
        "        response = chat.send_message(user_input)\n",
        "        print(f\"\\nü§ñ Agent: {response.text}\\n\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        print(\"Try rephrasing your question.\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZ4aAFjqgoF4"
      },
      "source": [
        "---\n",
        "\n",
        "## üéØ Challenge: Test Your Agent's Memory\n",
        "\n",
        "Try this conversation sequence to test if your agent remembers context:\n",
        "\n",
        "1. \"I have a table called customers with columns: id, name, email, created_at\"\n",
        "2. \"How do I find duplicate emails?\"\n",
        "3. \"What if I want to keep the oldest record?\"\n",
        "\n",
        "**Did the agent remember your table structure?**\n",
        "\n",
        "Try other multi-turn conversations relevant to your work!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqalWAjHgoF4"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 5: What We've Built\n",
        "\n",
        "In this session you've created:\n",
        "\n",
        "‚úì **LLM Connection** - You can call Gemini API  \n",
        "‚úì **Single-turn Generation** - Ask questions, get answers  \n",
        "‚úì **Conversational Agent** - Remembers conversation history  \n",
        "‚úì **Interactive Interface** - Chat with your agent  \n",
        "\n",
        "**This is the foundation for all AI agents!**\n",
        "\n",
        "Everything else builds on these basics:\n",
        "- System prompts (Session 2)\n",
        "- Tool use (Session 3)\n",
        "- Multi-agent systems (Session 4)\n",
        "\n",
        "---\n",
        "\n",
        "## üìù Key Concepts\n",
        "\n",
        "**LLM (Large Language Model):**\n",
        "- Pre-trained AI model that understands and generates text\n",
        "- Examples: Gemini, GPT, Claude\n",
        "\n",
        "**Prompt:**\n",
        "- The input you send to the LLM\n",
        "- Can be a question, instruction, or context\n",
        "\n",
        "**Conversation History:**\n",
        "- Storing previous messages in the chat\n",
        "- Allows the agent to reference earlier context\n",
        "\n",
        "**Agent:**\n",
        "- An LLM with specific behaviour/purpose\n",
        "- Can be specialised through system prompts and tools\n",
        "\n",
        "---\n",
        "\n",
        "## üöÄ Next Steps\n",
        "\n",
        "**Before Session 2:**\n",
        "- Think about what data engineering tasks you'd like to automate\n",
        "- What would be useful in your actual work?\n",
        "\n",
        "**In Session 2 we'll:**\n",
        "- Add system prompts to specialise your agent\n",
        "- Create domain-specific agents (SQL helper, data quality checker, etc.)\n",
        "- Make agents give better, more focused responses\n",
        "\n",
        "---\n",
        "\n",
        "## üíæ Save This Notebook!\n",
        "\n",
        "**File ‚Üí Save a copy in Drive**\n",
        "\n",
        "Your API key is saved securely, and you can continue experimenting anytime!\n",
        "\n",
        "---\n",
        "\n",
        "## ü§î Reflection Questions\n",
        "\n",
        "Before we finish, think about:\n",
        "\n",
        "1. **What surprised you** about how simple this is?\n",
        "2. **Where in your work** could a conversational agent help?\n",
        "3. **What questions** would you want to ask your ideal data engineering assistant?\n",
        "\n",
        "Discuss with a colleague or make notes for yourself.\n",
        "\n",
        "---\n",
        "\n",
        "**Great work! See you in Session 2 where we'll make these agents much more powerful.** üéâ"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}